(window.webpackJsonp=window.webpackJsonp||[]).push([[29],{311:function(v,_,a){"use strict";a.r(_);var t=a(10),s=Object(t.a)({},(function(){var v=this,_=v._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[_("p",[v._v("给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，找出 a、b 两个文件共同的 URL。内存限制是 4G。")]),v._v(" "),_("p",[v._v("每个 URL 占 64B，那么 50 亿个 URL 占用的空间大小约为 320GB。由于内存大小只有 4G，因此，不可能一次性把所有 URL 加载到内存中处理。")]),v._v(" "),_("p",[v._v("因此可以采用分治策略，也就是把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。")]),v._v(" "),_("p",[v._v("首先遍历文件a，对遍历到的 URL 进行哈希取余 "),_("code",[v._v("hash(URL) % 1000")]),v._v("，根据计算结果把遍历到的 URL 存储到 a0, a1,a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。")]),v._v(" "),_("p",[v._v("这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。")]),v._v(" "),_("p",[v._v("接着遍历 "),_("code",[v._v("ai(i∈[0,999])")]),v._v("，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。")]),v._v(" "),_("p",[v._v("最后总结一下：")]),v._v(" "),_("ol",[_("li",[v._v("分而治之，进行哈希取余")]),v._v(" "),_("li",[v._v("对每个子文件进行 HashSet 统计")])]),v._v(" "),_("hr"),v._v(" "),_("p",[v._v("原作者：大彬")]),v._v(" "),_("p",[v._v("原文链接：https://www.topjavaer.cn/mass-data/3-find-same-url.html")]),v._v(" "),_("p",[v._v("站长略有修改")])])}),[],!1,null,null,null);_.default=s.exports}}]);